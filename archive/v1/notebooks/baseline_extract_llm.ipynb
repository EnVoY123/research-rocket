{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:24:44.187869Z",
     "start_time": "2024-05-19T11:24:44.142049Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:21:34.516356Z",
     "start_time": "2024-05-19T11:21:34.513868Z"
    }
   },
   "id": "d5ec38a4aba03b81",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0, file: interview_3.txt, length: 37995, tokens: 17444.\n",
      "index: 1, file: interview_2.txt, length: 33951, tokens: 15518.\n",
      "index: 2, file: interview_1.txt, length: 36025, tokens: 16647.\n",
      "index: 3, file: interview_5.txt, length: 50584, tokens: 23353.\n",
      "index: 4, file: interview_4.txt, length: 39364, tokens: 17577.\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"../data/demo\", glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "for num, doc in enumerate(documents):\n",
    "    print(\n",
    "        f\"index: {num}, file: {doc.metadata['source'][-15:]}, length: {len(doc.page_content)}, tokens: {len(encoding.encode(doc.page_content))}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:21:37.130468Z",
     "start_time": "2024-05-19T11:21:35.377641Z"
    }
   },
   "id": "f7ca385dfce9928e",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# interview_1 = documents[0].page_content\n",
    "# interview_2 = documents[1].page_content\n",
    "# interview_3 = documents[1].page_content\n",
    "# interview_4 = documents[1].page_content\n",
    "# interview_5 = documents[1].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:13:29.669007Z",
     "start_time": "2024-05-19T11:13:29.665470Z"
    }
   },
   "id": "a2b40225d2141350",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_haiku = ChatAnthropic(api_key=ANTHROPIC_API_KEY, model=\"claude-3-haiku-20240307\", max_tokens=4096, temperature=0.0)\n",
    "llm_haiku_t = ChatAnthropic(api_key=ANTHROPIC_API_KEY, model=\"claude-3-haiku-20240307\", max_tokens=4096, temperature=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:21:42.543850Z",
     "start_time": "2024-05-19T11:21:42.470443Z"
    }
   },
   "id": "32d7fd0ee559a056",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def asks_llm(llm: None, raw_data: list[str] = None, text_template: str = None, input_variables: list[str] = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param llm: \n",
    "    :param raw_interview: \n",
    "    :param text_template: \n",
    "    :param input_variables: \n",
    "    :param input_chain: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    upd = \"\\nAttention, this is important! Always send the full text of the response in Russian only. Thanks!\"\n",
    "\n",
    "    prompt = PromptTemplate(input_variables=input_variables, template=text_template + upd)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    input_chain = {}\n",
    "    for var, data in zip(input_variables, raw_data):\n",
    "        input_chain[var] = data\n",
    "\n",
    "    answer = chain.invoke(input_chain)\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:21:43.669968Z",
     "start_time": "2024-05-19T11:21:43.666457Z"
    }
   },
   "id": "e00ee8f3ece9420a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_llm_response(raw_answer):\n",
    "    metadata = {\n",
    "        \"model\": raw_answer.response_metadata[\"model\"],\n",
    "        \"input_tokens\": raw_answer.response_metadata[\"usage\"][\"input_tokens\"],\n",
    "        \"output_tokens\": raw_answer.response_metadata[\"usage\"][\"output_tokens\"]\n",
    "    }\n",
    "\n",
    "    content = json.loads(raw_answer.content)\n",
    "\n",
    "    return metadata, content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:21:44.547009Z",
     "start_time": "2024-05-19T11:21:44.544092Z"
    }
   },
   "id": "632f76b14367593c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main(interview_number, interview_raw_data):\n",
    "    answer_1 = asks_llm(\n",
    "        llm=llm_haiku, raw_data=[interview_raw_data],\n",
    "        text_template=\"\"\"\n",
    "        Пожалуйста, внимательно прочтите предоставленное вам интервью и следуйте приведенным ниже пошаговым инструкциям: <interview>{raw_interview}</interview>\n",
    "\n",
    "        Пошаговые инструкции:\n",
    "        1. Используйте только информацию, предоставленную вам в интервью, не придумывайте ничего самостоятельно.\n",
    "        2. Выделите основные разделы или фазы интервью. Например: введение, общие вопросы, основные темы, заключительные замечания.\n",
    "        3. Для каждого выделенного основного раздела приведите не более пяти цитат из вопросов в этом разделе. Важно, чтобы цитаты были короткими, но описывали этот раздел как можно полнее. Используйте только цитаты из вопросов интервьюера. Если в цитате встречается имя интервьюера или респондента, то удалите это имя.\n",
    "        4 Верните окончательный ответ в формате json, где ключом будет название основного раздела интервью, а значениями список всех выбранных вами цитат. \n",
    "        \"\"\",\n",
    "        input_variables=[\"raw_interview\"],\n",
    "    )\n",
    "    answer_1_metadata, answer_1_content = extract_llm_response(answer_1)\n",
    "    \n",
    "    time.sleep(60)\n",
    "    answer_2 = asks_llm(\n",
    "        llm=llm_haiku, raw_data=[interview_raw_data],\n",
    "        text_template=\"\"\"\n",
    "        Пожалуйста, внимательно прочтите предоставленное вам интервью и следуйте приведенным ниже пошаговым инструкциям: <interview>{raw_interview}</interview>\n",
    "\n",
    "        Пошаговые инструкции:\n",
    "        1. Используйте только информацию, предоставленную вам в интервью, не придумывайте ничего самостоятельно.\n",
    "        2. Выделите ключевые фразы, словосочетания или предложения, отражающие важные идеи, концепции или опыт. \n",
    "        3. Присвоите получившимся выделенным сегментам короткие ярлыки. Используя короткие описательные фразы или слова, которые обобщают основную идею.\n",
    "        4. Верните окончательный ответ в формате json, где ключом будет короткий ярлык, а значениями будут описания этого кода-ярлыка.\n",
    "        \"\"\",\n",
    "        input_variables=[\"raw_interview\"],\n",
    "    )\n",
    "    answer_2_metadata, answer_2_content = extract_llm_response(answer_2)\n",
    "    answer_2_codes = \", \".join(list(answer_2_content.keys()))\n",
    "\n",
    "    time.sleep(60)\n",
    "    answer_3 = asks_llm(\n",
    "        llm=llm_haiku, raw_data=[interview_raw_data, answer_2_codes],\n",
    "        text_template=\"\"\"\n",
    "        Пожалуйста, внимательно прочтите предоставленное вам интервью и следуйте приведенным ниже пошаговым инструкциям: <interview>{raw_interview}</interview>\n",
    "\n",
    "        Пошаговые инструкции:\n",
    "        1. Используйте только информацию, предоставленную вам в интервью, не придумывайте ничего самостоятельно.\n",
    "        2. Просмотрите тематические коды <interview_code>{interview_code}</interview_code> и найдите среди них сходства или взаимосвязи.\n",
    "        3. Сгруппируйте связанные тематические коды в более широкие категории или темы, которые охватывают основные темы, обсуждавшиеся в интервью.\n",
    "        4. Определите любые подтемы в рамках каждой основной темы, чтобы обеспечить более детальное понимание тем.\n",
    "        5. Верните окончательный ответ в формате json, где ключами будут основные темы, а значениями список подтем. \n",
    "        \"\"\",\n",
    "        input_variables=[\"raw_interview\", \"interview_code\"]\n",
    "    )\n",
    "    answer_3_metadata, answer_3_content = extract_llm_response(answer_3)\n",
    "\n",
    "    time.sleep(60)\n",
    "    answer_4 = asks_llm(\n",
    "        llm=llm_haiku, raw_data=[interview_raw_data],\n",
    "        text_template=\"\"\"\n",
    "        Пожалуйста, внимательно прочтите предоставленное вам интервью и следуйте приведенным ниже пошаговым инструкциям: <interview>{raw_interview}</interview>\n",
    "\n",
    "        Пошаговые инструкции:\n",
    "        1. Используйте только информацию, предоставленную вам в интервью, не придумывайте ничего самостоятельно.\n",
    "        2. Составьте список всех конкретных проблем, упомянутых респондентами.\n",
    "        2. Подсчитайте, сколько раз упоминалась каждая проблема. \n",
    "        3. Верните окончательный ответ в формате json, где ключами будут проблемы, а значениями количество упоминаний проблем в интервью. \n",
    "        \"\"\",\n",
    "        input_variables=[\"raw_interview\"]\n",
    "    )\n",
    "    answer_4_metadata, answer_4_content = extract_llm_response(answer_4)\n",
    "    answer_4_problems = \", \".join(list(answer_4_content.keys()))\n",
    "\n",
    "    time.sleep(60)\n",
    "    answer_5 = asks_llm(\n",
    "        llm=llm_haiku, raw_data=[interview_raw_data, answer_4_problems],\n",
    "        text_template=\"\"\"\n",
    "        Пожалуйста, внимательно прочтите предоставленное вам интервью и следуйте приведенным ниже пошаговым инструкциям: <interview>{raw_interview}</interview>\n",
    "\n",
    "        Пошаговые инструкции:\n",
    "        1. Используйте только информацию, предоставленную вам в интервью, не придумывайте ничего самостоятельно.\n",
    "        2. Просмотрите список проблем <interview_problems>{interview_problems}</interview_problems> и найдите среди них сходства или взаимосвязи.\n",
    "        2. Сгруппируйте связанные проблемы в более широкие типы проблем, которые охватывают различные упомянутые проблемы в интервью. \n",
    "        \n",
    "        4. Верните окончательный ответ в формате json, где ключами будут широкие типы проблем, а значениями списки проблем входящих в них.\n",
    "        \"\"\",\n",
    "        input_variables=[\"raw_interview\", \"interview_problems\"]\n",
    "    )\n",
    "    answer_5_metadata, answer_5_content = extract_llm_response(answer_5)\n",
    "    answer_5_group_problems = \", \".join(list(answer_5_content.keys()))\n",
    "\n",
    "    time.sleep(60)\n",
    "    answer_6 = asks_llm(\n",
    "        llm=llm_haiku_t, raw_data=[interview_raw_data, answer_5_group_problems],\n",
    "        text_template=\"\"\"\n",
    "        Пожалуйста, внимательно прочтите предоставленное вам интервью и категории проблем, которые были выявлены при первичном анализе данного интервью: <interview>{raw_interview}</interview> and <interview_problems>{interview_problems_categories}</interview_problems> \n",
    "\n",
    "\n",
    "        Пошаговые инструкции:\n",
    "        1. Используйте только информацию, предоставленную вам в интервью и в выявленных категориях проблем, не придумывайте ничего самостоятельно.\n",
    "        \n",
    "        2. Проанализируйте темы и частоту возникновения проблем и их категории. Дайте ответ в форме вложенного json, верхнеуровневым ключом будет – \"reflections\", вложенными ключами будут категория проблем, а значениями будет список содержащий результаты проведенного анализа.  \n",
    "        \n",
    "        3. Проанализируйте, что результаты \"reflections\" говорят об опыте, взглядах или трудностях респондента. Дайте ответ в форме вложенного json, верхнеуровневым ключом будет – \"results\", вложенными ключами будут категория опыта, а значениями будет список содержащий результаты проведенного анализа.\n",
    "        \n",
    "        4. Обратите внимание на любые неожиданные закономерности, связи или контрасты в данных. Поделитесь этими наблюдениями. Дайте ответ в форме вложенного json, верхнеуровневым ключом будет – \"unexpected\", вложенными ключами будут категория контрастов, а значениями будет список содержащий результаты проведенного анализа.\n",
    "        \n",
    "        5. Разработайте предварительные объяснения или гипотезы о том, почему возникли определенные темы или проблемы и как они могут быть связаны друг с другом или с более широкими контекстуальными факторами. Дайте ответ в форме json, где будет только один ключ – \"hypothesis\", а значениями будет список содержащий результаты проведенного анализа.\n",
    "        \n",
    "        6. Рассмотрите альтернативные объяснения и контрпримеры, чтобы прояснить гипотезы сформированные в разделе \"hypothesis\". Дайте ответ в форме json, где будет только один ключ – \"alternatives\", а значениями будет список содержащий результаты проведенного анализа.\n",
    "        \n",
    "        7. Укажите области, в которых могут потребоваться дополнительные исследования или анализ для подтверждения ваших идей из раздела \"alternatives\". Дайте ответ в форме json, где будет только один ключ – \"additional\", а значениями будет список содержащий результаты проведенного анализа.\n",
    "        \"\"\",\n",
    "        input_variables=[\"raw_interview\", \"interview_problems_categories\"]\n",
    "    )\n",
    "    answer_6_metadata, answer_6_content = extract_llm_response(answer_6)\n",
    "\n",
    "    results = {\n",
    "        \"step_1\": answer_1_content,\n",
    "        \"step_2\": answer_2_content,\n",
    "        \"step_3\": answer_3_content,\n",
    "        \"step_4\": answer_4_content,\n",
    "        \"step_5\": answer_5_content,\n",
    "        \"step_6\": answer_6_content,\n",
    "    }\n",
    "\n",
    "    with open(f\"../data/demo/results_interview_{interview_number}.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False)\n",
    "    time.sleep(60)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:26:23.453738Z",
     "start_time": "2024-05-19T11:26:23.444990Z"
    }
   },
   "id": "88f40c5b4808cc56",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T19:36:12.998496Z",
     "start_time": "2024-05-18T19:36:12.996836Z"
    }
   },
   "id": "4344993ebd1f6cdf",
   "execution_count": 239
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9834d51974554cb09da774041e0e68c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 37 column 100 (char 2622)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m num, doc \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(documents)):\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43minterview_number\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msource\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterview_raw_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpage_content\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 110\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(interview_number, interview_raw_data)\u001B[0m\n\u001B[1;32m     86\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m60\u001B[39m)\n\u001B[1;32m     87\u001B[0m answer_6 \u001B[38;5;241m=\u001B[39m asks_llm(\n\u001B[1;32m     88\u001B[0m     llm\u001B[38;5;241m=\u001B[39mllm_haiku_t, raw_data\u001B[38;5;241m=\u001B[39m[interview_raw_data, answer_5_group_problems],\n\u001B[1;32m     89\u001B[0m     text_template\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    108\u001B[0m     input_variables\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_interview\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterview_problems_categories\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    109\u001B[0m )\n\u001B[0;32m--> 110\u001B[0m answer_6_metadata, answer_6_content \u001B[38;5;241m=\u001B[39m \u001B[43mextract_llm_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43manswer_6\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m results \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_1\u001B[39m\u001B[38;5;124m\"\u001B[39m: answer_1_content,\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_2\u001B[39m\u001B[38;5;124m\"\u001B[39m: answer_2_content,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_6\u001B[39m\u001B[38;5;124m\"\u001B[39m: answer_6_content,\n\u001B[1;32m    119\u001B[0m }\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/demo/results_interview_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minterview_number\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "Cell \u001B[0;32mIn[15], line 8\u001B[0m, in \u001B[0;36mextract_llm_response\u001B[0;34m(raw_answer)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_llm_response\u001B[39m(raw_answer):\n\u001B[1;32m      2\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: raw_answer\u001B[38;5;241m.\u001B[39mresponse_metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: raw_answer\u001B[38;5;241m.\u001B[39mresponse_metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musage\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: raw_answer\u001B[38;5;241m.\u001B[39mresponse_metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musage\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      6\u001B[0m     }\n\u001B[0;32m----> 8\u001B[0m     content \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_answer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m metadata, content\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:353\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    350\u001B[0m \n\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting ',' delimiter: line 37 column 100 (char 2622)"
     ]
    }
   ],
   "source": [
    "for num, doc in tqdm(enumerate(documents)):\n",
    "    main(interview_number=str(doc.metadata['source'][-5:-4]), interview_raw_data=doc.page_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:55:14.366798Z",
     "start_time": "2024-05-19T11:28:31.749820Z"
    }
   },
   "id": "2648c2dd6f2b3d61",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_haiku' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mllm_haiku\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'llm_haiku' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:37:40.439691Z",
     "start_time": "2024-05-22T10:37:40.361695Z"
    }
   },
   "id": "5537d90837f68335",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T11:17:36.665637Z",
     "start_time": "2024-05-19T11:17:36.664267Z"
    }
   },
   "id": "78d40d6570fd4662",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m prompt \u001B[38;5;241m=\u001B[39m ChatPromptTemplate\u001B[38;5;241m.\u001B[39mfrom_template(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtell me a short joke about \u001B[39m\u001B[38;5;132;01m{topic}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m output_parser \u001B[38;5;241m=\u001B[39m StrOutputParser()\n\u001B[0;32m----> 7\u001B[0m chain \u001B[38;5;241m=\u001B[39m prompt \u001B[38;5;241m|\u001B[39m \u001B[43mmodel\u001B[49m \u001B[38;5;241m|\u001B[39m output_parser\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# chain.invoke({\"topic\": \"ice cream\"})\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T10:37:59.756659Z",
     "start_time": "2024-05-22T10:37:59.501096Z"
    }
   },
   "id": "fc3d5e31eddb0491",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "798e851b8ea25b40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
